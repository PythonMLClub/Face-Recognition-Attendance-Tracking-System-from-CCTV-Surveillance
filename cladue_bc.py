import cv2
from ultralytics import YOLO
import cvzone
import numpy as np
import csv
import threading
import queue
import time
import json
import os
import faiss
import pickle
import sys

# Performance optimizations
cv2.setNumThreads(4)
cv2.setUseOptimized(True)
import torch
torch.set_num_threads(4)

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["OMP_NUM_THREADS"] = "1"

from datetime import datetime
from utils.embedding_model import ArcFaceEmbedder
from utils.db_handler import get_employee_details, get_employees_without_embedding, update_embedding, get_all_embeddings, log_attendance
from utils.detect_face import detect_faces

# Adjust sys.path for module imports
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Load config
try:
    with open('config.json', 'r') as config_file:
        config = json.load(config_file)
        video_config = config['entry_video']
        RTSP_URL = video_config['path']
        WIDTH = video_config.get('width', 1020)
        HEIGHT = video_config.get('height', 600)
        FPS = video_config.get('fps', 15)
        FRAME_INTERVAL = video_config.get('frame_interval', 5)
        OUTPUT_PATH = video_config.get('output_path', 'output_video_entry.mp4')
        
        # FIXED: Handle both absolute pixels and ratio values
        line_position_value = video_config.get('line_position', 0.4)
        
        # Check if it's absolute pixels (>1) or ratio (0-1)
        if line_position_value > 1:
            # Absolute pixel value
            ACTUAL_LINE_Y = int(line_position_value)
            LINE_Y_RATIO = line_position_value / HEIGHT
            print(f"‚úÖ Using absolute line position: {ACTUAL_LINE_Y} pixels")
        else:
            # Ratio value (0-1)
            LINE_Y_RATIO = line_position_value
            ACTUAL_LINE_Y = int(HEIGHT * LINE_Y_RATIO)
            print(f"‚úÖ Using ratio line position: {LINE_Y_RATIO} ({ACTUAL_LINE_Y} pixels)")
        
        DISPLAY_LABEL = video_config.get('display_label', 'ENTRY')
        CAMERA_ID = video_config.get('camera_id', 'CAM1')
        EVENT_TYPE = video_config.get('event_type', 'Entry')
        LOCATION = video_config.get('location', 'Entry Gate')
        MAX_RETRIES = video_config.get('max_retries', 5)
        SIMILARITY_THRESHOLD = config.get('similarity_threshold', 1.5)
        
        print(f"‚úÖ Config loaded: Line position = {LINE_Y_RATIO} ({ACTUAL_LINE_Y} pixels)")
        
except FileNotFoundError:
    print("‚ùå Error: config.json file not found.")
    exit(1)
except KeyError as e:
    print(f"‚ùå Error: Missing key in config.json: {e}")
    exit(1)
except Exception as e:
    print(f"‚ùå Error reading config.json: {e}")
    exit(1)

# Load YOLOv8 model
model = YOLO('models/yolov8n.pt')
face_model = YOLO('models/yolov8m-face-lindevs.pt')  # For face detection fallback

# Initialize ArcFace embedder
embedder = ArcFaceEmbedder()

# Disable OpenMP in FAISS
faiss.omp_set_num_threads(1)

# Track previous center positions
track_history = {}
in_count = 0
frame_queue = queue.Queue(maxsize=60)  # Increased queue size
stop_thread = False

# FIXED: Global variables for better attendance tracking
employee_persistence = {}  # Initialize the global dictionary
attendance_logged_this_session = set()  # FIXED: Track logged employees in this session
unique_id_counter = 50000  # FIXED: Counter for truly unique IDs

DEDUPLICATION_WINDOW_SECONDS = 60
TRACK_EXPIRY_FRAMES = 45  # Reduced for better multi-person handling

# Line crossing detection parameters - OPTIMIZED FOR MULTIPLE PEOPLE
CROSSING_TOLERANCE = 8  # Reduced from 15 for better multi-person detection
MIN_HISTORY_FOR_CROSSING = 2  # Minimum positions needed for crossing detection

def get_unique_track_id():
    """Generate truly unique track ID"""
    global unique_id_counter
    unique_id_counter += 1
    return unique_id_counter

def detect_line_crossing_multiple(track_history, track_id, actual_line_y, frame_count):
    """Enhanced line crossing detection for multiple people simultaneously"""
    
    if track_id not in track_history:
        return False, None, None
        
    if track_history[track_id]['crossed']:
        return track_history[track_id]['crossed'], None, None
    
    history = track_history[track_id]['history']
    
    if len(history) < MIN_HISTORY_FOR_CROSSING:
        return False, None, None
    
    # Get recent positions
    prev_y = history[-2] if len(history) >= 2 else None
    curr_y = history[-1]
    
    crossed = False
    crossing_method = None
    
    # Method 1: Direct crossing with smaller tolerance for crowded scenes
    if prev_y is not None:
        if (prev_y < actual_line_y - CROSSING_TOLERANCE and 
            curr_y > actual_line_y + CROSSING_TOLERANCE):
            crossed = True
            crossing_method = "direct"
            print(f"üéØ DIRECT CROSSING: TrackID {track_id} - PrevY:{prev_y}, CurrY:{curr_y}, LineY:{actual_line_y}")
    
    # Method 2: Simple center crossing (most reliable for multiple people)
    if not crossed and prev_y is not None:
        if prev_y < actual_line_y and curr_y > actual_line_y:
            crossed = True
            crossing_method = "simple"
            print(f"üéØ SIMPLE CROSSING: TrackID {track_id} - PrevY:{prev_y}, CurrY:{curr_y}")
    
    # Method 3: Progressive crossing for partial occlusion
    if not crossed and len(history) >= 3:
        recent_positions = history[-3:]
        positions_above = sum(1 for y in recent_positions if y < actual_line_y)
        positions_below = sum(1 for y in recent_positions if y > actual_line_y)
        
        if positions_above >= 1 and positions_below >= 1 and curr_y > actual_line_y:
            crossed = True
            crossing_method = "progressive"
            print(f"üéØ PROGRESSIVE CROSSING: TrackID {track_id} - Above:{positions_above}, Below:{positions_below}")
    
    # Method 4: Trend-based crossing (for smooth movement)
    if not crossed and len(history) >= 4:
        # Check if person is moving downward across the line
        recent_trend = history[-1] - history[-4]  # Overall movement direction
        if (recent_trend > 10 and  # Moving downward
            prev_y <= actual_line_y + 5 and curr_y >= actual_line_y - 5):
            crossed = True
            crossing_method = "trend"
            print(f"üéØ TREND CROSSING: TrackID {track_id} - Trend:{recent_trend}")
    
    # Method 5: Alternative crossing pattern detection
    if not crossed and len(history) >= 4:
        # Look for crossing pattern in last 4 positions
        for i in range(len(history) - 1):
            if i < len(history) - 1:
                y1 = history[i]
                y2 = history[i + 1]
                if y1 < actual_line_y < y2:  # Crossing detected
                    crossed = True
                    crossing_method = "pattern"
                    print(f"üéØ PATTERN CROSSING: TrackID {track_id} - Found crossing between positions {i} and {i+1}")
                    break
    
    # Debug information for troubleshooting
    if frame_count % 30 == 0 and len(history) > 0:  # Print debug every 30 frames
        print(f"üîç TrackID {track_id} Frame {frame_count}: History={history[-5:]}, LineY={actual_line_y}, Crossed={crossed}")
    
    return crossed, prev_y, curr_y

def manage_crowded_tracks(track_history, frame_count):
    """Better track management for crowded scenes"""
    
    # Identify tracks to clean up
    expired_tracks = []
    unmatched_old_tracks = []
    
    for track_id, data in track_history.items():
        frames_since_seen = frame_count - data['last_seen_frame']
        
        if frames_since_seen > TRACK_EXPIRY_FRAMES:
            expired_tracks.append(track_id)
        elif frames_since_seen > 20 and not data.get('face_matched', False):
            unmatched_old_tracks.append((track_id, frames_since_seen))
    
    # Remove expired tracks
    for track_id in expired_tracks:
        del track_history[track_id]
        print(f"üóëÔ∏è Removed expired TrackID {track_id}")
    
    # If too many tracks, remove oldest unmatched ones
    if len(track_history) > 20:  # Limit total tracks
        unmatched_old_tracks.sort(key=lambda x: x[1], reverse=True)
        for track_id, _ in unmatched_old_tracks[:5]:  # Remove oldest 5
            if track_id in track_history:
                del track_history[track_id]
                print(f"üóëÔ∏è Removed crowded TrackID {track_id}")

def enhanced_person_detection(frame, model, face_model, frame_count):
    """FIXED: Enhanced person detection with unique track ID management"""
    
    # Primary detection with optimized parameters
    person_results = model.track(
        frame, 
        persist=True, 
        classes=[0],  # Person class
        conf=0.25,    # LOWERED from 0.5 - detects more people
        iou=0.5,      # NMS IoU threshold - prevents over-suppression
        max_det=15,   # Allow more detections per frame
        tracker="bytetrack.yaml"  # Use ByteTrack for better multi-object tracking
    )
    
    boxes = []
    person_ids = []
    confidences = []
    
    # Get person detections
    if person_results[0].boxes.id is not None:
        person_ids = person_results[0].boxes.id.cpu().numpy().astype(int).tolist()
        boxes = person_results[0].boxes.xyxy.cpu().numpy().astype(int).tolist()
        confidences = person_results[0].boxes.conf.cpu().numpy().tolist()
        print(f"Frame {frame_count}: Person Detection - Found {len(boxes)} people with IDs: {person_ids}")
    
    # FIXED: Enhanced fallback with unique IDs
    if len(boxes) < 3:  # If fewer than 3 people detected, try face detection
        face_results = face_model(frame, conf=0.15, iou=0.4, max_det=10)
        
        if face_results[0].boxes is not None and len(face_results[0].boxes) > 0:
            face_boxes = face_results[0].boxes.xyxy.cpu().numpy().astype(int).tolist()
            face_confidences = face_results[0].boxes.conf.cpu().numpy().tolist()
            
            # Convert face boxes to person-sized boxes (expand face to estimate person)
            expanded_boxes = []
            for i, (fx1, fy1, fx2, fy2) in enumerate(face_boxes):
                # Expand face box to estimate person box
                face_width = fx2 - fx1
                face_height = fy2 - fy1
                
                # Estimate person box (face is typically 1/8 of person height)
                person_width = int(face_width * 2.5)
                person_height = int(face_height * 6)
                
                # Calculate person box centered on face
                center_x = (fx1 + fx2) // 2
                center_y = (fy1 + fy2) // 2
                
                px1 = max(0, center_x - person_width // 2)
                py1 = max(0, center_y - person_height // 3)  # Face in upper part
                px2 = min(frame.shape[1], center_x + person_width // 2)
                py2 = min(frame.shape[0], center_y + (2 * person_height) // 3)
                
                expanded_boxes.append([px1, py1, px2, py2])
            
            # Add face-derived detections that don't overlap with existing person detections
            for i, face_box in enumerate(expanded_boxes):
                fx1, fy1, fx2, fy2 = face_box
                
                # Check for overlap with existing person detections
                overlap = False
                for px1, py1, px2, py2 in boxes:
                    # Calculate IoU
                    x1 = max(fx1, px1)
                    y1 = max(fy1, py1)
                    x2 = min(fx2, px2)
                    y2 = min(fy2, py2)
                    
                    if x1 < x2 and y1 < y2:
                        intersection = (x2 - x1) * (y2 - y1)
                        area1 = (fx2 - fx1) * (fy2 - fy1)
                        area2 = (px2 - px1) * (py2 - py1)
                        union = area1 + area2 - intersection
                        iou = intersection / union if union > 0 else 0
                        
                        if iou > 0.3:  # Significant overlap
                            overlap = True
                            break
                
                # Add non-overlapping face-derived detection with UNIQUE ID
                if not overlap:
                    boxes.append(face_box)
                    new_id = get_unique_track_id()  # FIXED: Use truly unique ID
                    person_ids.append(new_id)
                    confidences.append(face_confidences[i] * 0.8)  # Lower confidence
                    print(f"Added face-derived person: ID {new_id} at {face_box}")
    
    # ADDITIONAL FALLBACK: Lower confidence person detection
    if len(boxes) < 2:  # If still not enough detections
        low_conf_results = model(frame, conf=0.15, classes=[0], max_det=15)
        if low_conf_results[0].boxes is not None and len(low_conf_results[0].boxes) > 0:
            low_conf_boxes = low_conf_results[0].boxes.xyxy.cpu().numpy().astype(int).tolist()
            low_conf_confidences = low_conf_results[0].boxes.conf.cpu().numpy().tolist()
            
            # Filter out boxes that are too small (likely false positives)
            for i, box in enumerate(low_conf_boxes):
                x1, y1, x2, y2 = box
                width = x2 - x1
                height = y2 - y1
                if width > 30 and height > 60:  # Minimum person size
                    # Check for overlap with existing detections
                    overlap = False
                    for existing_box in boxes:
                        ex1, ey1, ex2, ey2 = existing_box
                        if (abs(x1 - ex1) < 50 and abs(y1 - ey1) < 50):
                            overlap = True
                            break
                    
                    if not overlap:
                        boxes.append(box)
                        new_id = get_unique_track_id()  # FIXED: Use truly unique ID
                        person_ids.append(new_id)
                        confidences.append(low_conf_confidences[i] * 0.6)
                        print(f"Added low-conf person: ID {new_id}")
    
    # Ensure we have confidence scores for all detections
    while len(confidences) < len(boxes):
        confidences.append(0.5)  # Default confidence
    
    return boxes, person_ids, confidences
def add_debug_visualization(frame, track_history, actual_line_y, frame_count):
    """FIXED: Enhanced debug visualization with better visibility."""
    
    # FIXED: Thicker, more visible entry line
    cv2.line(frame, (0, actual_line_y), (frame.shape[1], actual_line_y), (0, 255, 255), 6)  # Thicker line
    
    # FIXED: More visible line label
    line_label = f"ENTRY LINE Y={actual_line_y}"
    label_size = cv2.getTextSize(line_label, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 3)[0]
    cv2.rectangle(frame, (10, actual_line_y - 45), (20 + label_size[0], actual_line_y - 5), 
                 (0, 255, 255), cv2.FILLED)
    cv2.putText(frame, line_label, (15, actual_line_y - 20), 
               cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0), 3)
    
    # FIXED: Enhanced stats display with bigger text
    active_tracks = sum(1 for data in track_history.values() if len(data['history']) > 0)
    recognized_count = sum(1 for data in track_history.values() if data['face_matched'])
    unknown_count = active_tracks - recognized_count
    
    # FIXED: Larger stats box
    stats_bg_width = 400
    stats_bg_height = 180
    cv2.rectangle(frame, (10, 10), (10 + stats_bg_width, 10 + stats_bg_height), 
                 (0, 0, 0), cv2.FILLED)  # Black background
    cv2.rectangle(frame, (10, 10), (10 + stats_bg_width, 10 + stats_bg_height), 
                 (255, 255, 255), 3)  # White border
    
    # FIXED: Larger, more visible stats text
    cv2.putText(frame, f"Frame: {frame_count}", (20, 40), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    cv2.putText(frame, f"Active Tracks: {active_tracks}", (20, 70), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    cv2.putText(frame, f"Recognized: {recognized_count}", (20, 100), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
    cv2.putText(frame, f"Unknown: {unknown_count}", (20, 130), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
    cv2.putText(frame, f"Total Entries: {in_count}", (20, 160), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)

# Generate embeddings for employees
def generate_embeddings():
    records = get_employees_without_embedding()
    if not records:
        print("No employees without embeddings found.")
        return {}

    employee_embeddings = {}
    for emp_id, image_blob in records:
        try:
            image = np.frombuffer(image_blob, dtype=np.uint8)
            img = cv2.imdecode(image, cv2.IMREAD_COLOR)
            if img is None:
                print(f"‚ùå Failed to decode image for Employee ID: {emp_id}")
                continue

            faces = detect_faces(img)
            if not faces:
                print(f"‚ùå No face found for Employee ID: {emp_id}")
                continue

            face_crop, coords = faces[0]
            embedding = embedder.get_embedding(face_crop)
            if embedding is None:
                print(f"‚ùå Failed to generate embedding for Employee ID: {emp_id}")
                continue

            employee_embeddings[emp_id] = embedding
            success, message = update_embedding(emp_id, embedding)
            print(f"{'‚úÖ' if success else '‚ùå'} {message} for Employee ID: {emp_id}")
        except Exception as e:
            print(f"‚ùå Error processing Employee ID {emp_id}: {e}")

    print("‚úÖ Embeddings generation completed.")
    return employee_embeddings

# Build FAISS index
def build_faiss_index():
    try:
        records = get_all_embeddings()
        if not records:
            print("‚ùå No embeddings found in the database.")
            return None, None

        embeddings = []
        employee_ids = []
        for emp_id, embedding_blob in records:
            embedding = np.frombuffer(embedding_blob, dtype=np.float32)
            if embedding.shape[0] == 512:
                embeddings.append(embedding)
                employee_ids.append(str(emp_id))
            else:
                print(f"‚ùå Invalid embedding for Employee ID: {emp_id}")

        if not embeddings:
            print("‚ùå No valid embeddings to build FAISS index.")
            return None, None

        embeddings = np.array(embeddings).astype('float32')
        index = faiss.IndexFlatL2(512)
        index.add(embeddings)

        faiss.write_index(index, "faiss_index.bin")
        with open("employee_ids.pkl", "wb") as f:
            pickle.dump(employee_ids, f)
        print("‚úÖ FAISS index and employee IDs saved.")
        return index, employee_ids
    except Exception as e:
        print(f"‚ùå Error building FAISS index: {e}")
        return None, None
def capture_frames(rtsp_url):
    """FIXED: Thread function to capture frames with better control."""
    retry_count = 0
    while not stop_thread:
        cap = cv2.VideoCapture(rtsp_url)
        
        # FIXED: Set buffer size to prevent frame accumulation
        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        
        if not cap.isOpened():
            print(f"Error: Could not open RTSP stream. Retrying ({retry_count+1}/{MAX_RETRIES})...")
            retry_count += 1
            if retry_count >= MAX_RETRIES:
                print("‚ùå Max retries reached. Exiting capture thread.")
                break
            time.sleep(2)
            continue
            
        retry_count = 0
        frame_capture_count = 0
        
        while not stop_thread:
            ret, frame = cap.read()
            if not ret:
                print("Warning: Failed to read frame. Reconnecting...")
                break
                
            frame_capture_count += 1
            
            # FIXED: Better frame queue management
            try:
                # Clear old frames if queue is full
                while frame_queue.qsize() >= 3:  # Keep only 3 frames max
                    try:
                        frame_queue.get_nowait()
                    except queue.Empty:
                        break
                
                frame_queue.put_nowait(frame)
                
                if frame_capture_count % 30 == 0:  # Log every 30 frames
                    print(f"üìπ Captured {frame_capture_count} frames, queue size: {frame_queue.qsize()}")
                    
            except queue.Full:
                print("‚ö†Ô∏è Frame queue full, dropping frame")
                
            # FIXED: Small delay to prevent overwhelming
            time.sleep(0.01)
            
        cap.release()

# Start capture thread
capture_thread = threading.Thread(target=capture_frames, args=(RTSP_URL,), daemon=True)
capture_thread.start()

def check_attendance_exists_today(employee_id):
    """FIXED: Check if employee already has attendance logged today"""
    
    from datetime import datetime
    
    try:
        # This is a placeholder - implement based on your database
        # For now, we'll use the session tracking
        today_key = f"{employee_id}_{datetime.now().strftime('%Y-%m-%d')}"
        return employee_id in attendance_logged_this_session
        
    except Exception as e:
        print(f"‚ùå Error checking existing attendance: {e}")
        return False  # If error, proceed with logging

def log_employee_attendance(track_id, track_history, frame_count):
    """FIXED: Improved attendance logging with better duplicate prevention"""
    
    global attendance_logged_this_session
    
    attendance_logged = track_history[track_id]['attendance_logged']
    
    if (track_history[track_id]['crossed'] and 
        track_history[track_id]['face_matched'] and 
        track_history[track_id]['employee_id'] and
        not attendance_logged):
        
        current_time = datetime.now()
        employee_id = track_history[track_id]['employee_id']
        employee_name = track_history[track_id]['name']
        
        print(f"üéØ CHECKING ATTENDANCE for Employee {employee_id} ({employee_name})")
        
        # FIXED: Check if this employee has already been logged in this session
        if employee_id in attendance_logged_this_session:
            print(f"‚ö†Ô∏è Employee {employee_id} ({employee_name}) already logged in this session. Skipping.")
            track_history[track_id]['attendance_logged'] = True
            return False, "Already logged this session"
        
        # FIXED: Additional check for today's attendance
        if check_attendance_exists_today(employee_id):
            print(f"‚ö†Ô∏è Employee {employee_id} already has attendance today. Skipping.")
            track_history[track_id]['attendance_logged'] = True
            attendance_logged_this_session.add(employee_id)
            return False, "Already logged today"
        
        # Log attendance
        timestamp = current_time
        confidence_score = track_history[track_id]['detection_confidence']
        
        success, message = log_attendance(employee_id, timestamp, CAMERA_ID, LOCATION, confidence_score, EVENT_TYPE)
        
        if success:
            track_history[track_id]['attendance_logged'] = True
            attendance_logged_this_session.add(employee_id)  # FIXED: Mark as logged in session
            
            print(f"‚úÖ ATTENDANCE SUCCESSFULLY LOGGED:")
            print(f"   Employee: {employee_id} ({employee_name})")
            print(f"   Time: {timestamp}")
            print(f"   Track ID: {track_id}")
            print(f"   Frame: {frame_count}")
            print(f"   Session Total: {len(attendance_logged_this_session)} employees logged")
            
            return True, "Successfully logged"
        else:
            print(f"‚ùå Failed to log attendance: {message}")
            return False, f"Database error: {message}"
    
    return False, "Conditions not met"

# 1. FIXED: Video FPS and Frame Rate Control
def process_video(video_source, frame_interval, similarity_threshold, index, employee_ids):
    global in_count, track_history, employee_persistence, attendance_logged_this_session

    # FIXED: Better FPS control for proper playback speed
    ACTUAL_FPS = 10  # Slower FPS for better visualization (was 15)
    FRAME_DELAY = 1.0 / ACTUAL_FPS  # Control processing speed
    
    # FIXED: Video properties for saving with controlled FPS
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(OUTPUT_PATH, fourcc, ACTUAL_FPS, (WIDTH, HEIGHT))

    # Verify VideoWriter initialization
    if not out.isOpened():
        print("Error: VideoWriter failed to initialize.")
        exit()
    print(f"‚úÖ VideoWriter initialized with: Codec=mp4v, FPS={ACTUAL_FPS}, Size=({WIDTH}x{HEIGHT})")

    # FIXED: Frame timing control
    last_frame_time = time.time()

    # Open CSV file to log the tracking data with enhanced columns
    with open('entry_track_log.csv', mode='w', newline='') as csvfile:
        fieldnames = ['Frame', 'TrackID', 'EmployeeID', 'Name', 'Department', 'CenterX', 'CenterY', 
                     'PrevY', 'CurrY', 'LineY', 'Crossed', 'FaceDetected', 'FaceMatched', 
                     'DistanceToLine', 'History', 'AttendanceLogged', 'DetectionMethod', 'LoggingReason', 'UniqueAttendanceCount']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        frame_count = 0
        try:
            while True:
                # FIXED: Frame timing control - ensure consistent frame rate
                current_time = time.time()
                time_since_last_frame = current_time - last_frame_time
                
                # Control frame processing speed
                if time_since_last_frame < FRAME_DELAY:
                    time.sleep(FRAME_DELAY - time_since_last_frame)
                
                # Get frame from queue
                try:
                    frame = frame_queue.get(timeout=1.0)
                except queue.Empty:
                    print("Warning: Frame queue empty, no new frames received.")
                    continue

                frame_count += 1
                last_frame_time = time.time()

                # Validate frame
                if frame is None or frame.shape[0] == 0 or frame.shape[1] == 0:
                    print(f"Warning: Invalid frame at frame_count {frame_count}")
                    continue

                # Resize frame
                frame = cv2.resize(frame, (WIDTH, HEIGHT))
                
                # FIXED: Create a copy for visualization to ensure all detections are visible
                display_frame = frame.copy()
                
                # Add debug visualization first
                add_debug_visualization(display_frame, track_history, ACTUAL_LINE_Y, frame_count)

                # ENHANCED PERSON DETECTION
                boxes, ids, confidences = enhanced_person_detection(frame, model, face_model, frame_count)

                print(f"Frame {frame_count}: Total detected people: {len(boxes)} with IDs: {ids}")

                # Process detections (person or face)
                if boxes:
                    for i, (box, track_id, confidence) in enumerate(zip(boxes, ids, confidences)):
                        x1, y1, x2, y2 = box
                        x1 = max(0, x1 - 5)
                        y1 = max(0, y1 - 5)
                        x2 = min(WIDTH, x2 + 5)
                        y2 = min(HEIGHT, y2 + 5)
                        if x1 >= x2 or y1 >= y2:
                            print(f"‚ö†Ô∏è Warning: Adjusted person bbox is invalid for TrackID {track_id}: ({x1}, {y1}, {x2}, {y2})")
                            continue
                        center_x = (x1 + x2) // 2
                        center_y = (y1 + y2) // 2

                        # ... [previous track history and crossing detection code remains same] ...

                        # FIXED: Enhanced face detection visualization - ALWAYS visible
                        should_process_face = True  # REMOVED frame skipping for better visualization
                        
                        if should_process_face:
                            person_crop = frame[y1:y2, x1:x2]
                            if person_crop.size == 0:
                                print(f"‚ö†Ô∏è Warning: person_crop is empty for TrackID {track_id} at Frame {frame_count}")
                                continue
                            person_crop_height, person_crop_width = person_crop.shape[:2]

                            faces = detect_faces(person_crop)
                            face_detected = bool(faces)
                            track_history[track_id]['face_detected'] = face_detected
                            print(f"Frame {frame_count}, TrackID {track_id}: Face detected = {face_detected}")

                            if face_detected:
                                face_crop, (fx1, fy1, fx2, fy2) = faces[0]
                                fx1 = max(0, fx1)
                                fy1 = max(0, fy1)
                                fx2 = min(person_crop_width, fx2)
                                fy2 = min(person_crop_height, fy2)
                                
                                if fx1 < fx2 and fy1 < fy2:
                                    track_history[track_id]['face_coords'] = (fx1, fy1, fx2, fy2)
                                    face_coords = (fx1, fy1, fx2, fy2)
                                    
                                    # FIXED: Draw face detection box IMMEDIATELY for visibility
                                    face_x1 = x1 + fx1
                                    face_y1 = y1 + fy1
                                    face_x2 = x1 + fx2
                                    face_y2 = y1 + fy2
                                    
                                    # ENHANCED: Thicker, more visible face box
                                    cv2.rectangle(display_frame, (face_x1, face_y1), (face_x2, face_y2), (0, 255, 255), 4)  # Yellow, thick
                                    cv2.putText(display_frame, "FACE", (face_x1, face_y1-10), 
                                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)

                                    # Face matching with FAISS
                                    if index is not None:
                                        embedding = embedder.get_embedding(face_crop)
                                        if embedding is not None:
                                            embedding = embedding / np.linalg.norm(embedding)
                                            D, I = index.search(np.array([embedding]).astype('float32'), 1)
                                            distance = D[0][0]
                                            print(f"Frame {frame_count}, TrackID {track_id}: Distance = {distance}, Threshold = {similarity_threshold}")
                                            
                                            if distance < similarity_threshold:
                                                employee_id = int(employee_ids[I[0][0]])
                                                face_matched = True
                                                name, department = get_employee_details(employee_id)
                                                
                                                # Update track history
                                                track_history[track_id]['employee_id'] = employee_id
                                                track_history[track_id]['face_matched'] = True
                                                track_history[track_id]['name'] = name
                                                track_history[track_id]['department'] = department
                                                
                                                # ENHANCED: Show matched face with green box
                                                cv2.rectangle(display_frame, (face_x1, face_y1), (face_x2, face_y2), (0, 255, 0), 4)  # Green, thick
                                                cv2.putText(display_frame, f"MATCHED: {name}", (face_x1, face_y1-10), 
                                                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                                                
                                                print(f"‚úÖ Frame {frame_count}, TrackID {track_id}: Matched Employee ID {employee_id} ({name})")
                                            else:
                                                # ENHANCED: Show unmatched face with red box
                                                cv2.rectangle(display_frame, (face_x1, face_y1), (face_x2, face_y2), (0, 0, 255), 4)  # Red, thick
                                                cv2.putText(display_frame, "UNKNOWN", (face_x1, face_y1-10), 
                                                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)

                        # ENHANCED: More visible person bounding boxes
                        if track_history[track_id]['face_matched'] and track_history[track_id]['name']:
                            # Green for recognized
                            person_color = (0, 255, 0)
                            label = f"{track_history[track_id]['name']} (ID: {track_history[track_id]['employee_id']})"
                        else:
                            # Red for unknown
                            person_color = (0, 0, 255)
                            label = f"Unknown (Track: {track_id})"
                        
                        # FIXED: Thicker person bounding box for better visibility
                        cv2.rectangle(display_frame, (x1, y1), (x2, y2), person_color, 5)
                        
                        # FIXED: More visible label background
                        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
                        cv2.rectangle(display_frame, (x1, y1-35), (x1 + label_size[0] + 10, y1), person_color, cv2.FILLED)
                        cv2.putText(display_frame, label, (x1 + 5, y1-10), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                        
                        # FIXED: Show crossing status prominently
                        if track_history[track_id]['crossed']:
                            cv2.putText(display_frame, "CROSSED LINE!", (x1, y2+25), 
                                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 3)

                # FIXED: Ensure every frame is saved to output video
                if display_frame is not None and display_frame.shape[0] > 0 and display_frame.shape[1] > 0:
                    out.write(display_frame)
                    print(f"‚úÖ Frame {frame_count} saved to video")
                else:
                    print(f"‚ùå Failed to save frame {frame_count}")

                # FIXED: Display with controlled speed
                cv2.imshow("YOLOv8 Multi-Person Entry Detection - FIXED VISUALIZATION", display_frame)

                # ESC to break
                if cv2.waitKey(1) & 0xFF == 27:
                    break

        except KeyboardInterrupt:
            print("Program interrupted, cleaning up...")
        finally:
            global stop_thread
            stop_thread = True
            capture_thread.join(timeout=2.0)
            out.release()
            cv2.destroyAllWindows()
            
            # FIXED: Enhanced final summary
            print("\n" + "="*60)
            print("üìä FINAL SESSION SUMMARY")
            print("="*60)
            print(f"‚úÖ Total Entry Count: {in_count}")
            print(f"‚úÖ Unique Employees with Attendance Logged: {len(attendance_logged_this_session)}")
            print(f"‚úÖ Employee IDs Logged: {list(attendance_logged_this_session)}")
            print(f"‚úÖ Video saved to: {OUTPUT_PATH}")
            print(f"‚úÖ CSV log saved to: entry_track_log.csv")
            print("="*60)

# Main
if __name__ == "__main__":
    print("üöÄ Starting FIXED Multi-Person Attendance System...")
    print(f"üìè Entry line position: {ACTUAL_LINE_Y} pixels ({LINE_Y_RATIO*100}% from top)")
    print(f"üéØ Crossing tolerance: ¬±{CROSSING_TOLERANCE} pixels")
    print(f"üìä Counting: ALL people who cross line (recognized + unknown)")
    print(f"üìù Attendance: Only logged for recognized employees (NO DUPLICATES)")
    print(f"üë• Multi-person optimizations: ENABLED")
    print(f"üîß Track ID management: FIXED with unique IDs")
    print(f"üõ°Ô∏è Duplicate prevention: ENHANCED with session tracking")
    print(f"‚ö° Performance optimizations: ENABLED")
    
    employee_embeddings = generate_embeddings()
    index, employee_ids = build_faiss_index()
    
    if index is None or employee_ids is None:
        print("‚ùå Failed to build FAISS index. Check database and embeddings.")
    else:
        print(f"‚úÖ FAISS index loaded with {len(employee_ids)} employees")
        process_video(
            video_source=RTSP_URL,
            frame_interval=FRAME_INTERVAL,
            similarity_threshold=SIMILARITY_THRESHOLD,
            index=index,
            employee_ids=employee_ids
        )